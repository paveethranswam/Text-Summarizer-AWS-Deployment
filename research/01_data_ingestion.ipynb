{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6baf8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import urllib\n",
    "from textSummarizer.logging import logger\n",
    "from zipfile import ZipFile\n",
    "from dataclasses import dataclass\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61b86df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\papu_\\\\OneDrive\\\\Desktop\\\\University Courses\\\\3rd Semester\\\\AWS ML Speciality\\\\Text-Summarizer-AWS-Deployment\\\\research'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1284b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07521ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\papu_\\\\OneDrive\\\\Desktop\\\\University Courses\\\\3rd Semester\\\\AWS ML Speciality\\\\Text-Summarizer-AWS-Deployment'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c176bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity.yaml\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_dir: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e86994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad = DataIngestionConfig('a', 'b', 'c', 'd')\n",
    "ad.unzip_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c2746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d80762f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories, get_size\n",
    "\n",
    "@dataclass\n",
    "class ConfigurationManager:\n",
    "    config_file_path: Path = CONFIG_FILE_PATH\n",
    "    params_file_path: Path = PARAMS_FILE_PATH\n",
    "    \n",
    "    # Calling this to not override default init and post init is called as the last function of default init\n",
    "    def __post_init__(self):\n",
    "        self.config = read_yaml(self.config_file_path) # CONFIG_FILE_PATH taken from constants\n",
    "        self.params = read_yaml(self.params_file_path) # PARAMS_FILE_PATH taken from constants\n",
    "        \n",
    "        create_directories([self.config.artifacts_path, self.config.data_ingestion.root_dir])\n",
    "        \n",
    "    def getDataIngestionConfig(self) -> DataIngestionConfig:\n",
    "        \n",
    "        data_ingestion_config = self.config.data_ingestion\n",
    "        \n",
    "        dic = DataIngestionConfig(root_dir = data_ingestion_config.root_dir, \n",
    "                                 source_dir = data_ingestion_config.source_dir, \n",
    "                                 local_data_file = data_ingestion_config.local_data_file, \n",
    "                                 unzip_dir = data_ingestion_config.unzip_dir)\n",
    "        \n",
    "        return dic\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c48ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc6c77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components creation\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, dic : DataIngestionConfig):\n",
    "        self.dic = dic\n",
    "        \n",
    "    # Download the data from url and save to the local file name and extract the zip contents, save them all in artifacts folder\n",
    "    def download_data(self):\n",
    "        if(not os.path.exists(self.dic.local_data_file)):\n",
    "            filename, header = urllib.request.urlretrieve(url = self.dic.source_dir, filename = self.dic.local_data_file)\n",
    "            logger.info('Data file {} downloaded, with return header {}'.format(filename, header))\n",
    "        else:\n",
    "            size = get_size(Path(self.dic.local_data_file))\n",
    "            logger.info('Data file {} already exists, with {}'.format(self.dic.local_data_file, size))\n",
    "    # Extract\n",
    "    def extract_zip(self):\n",
    "        with ZipFile(self.dic.local_data_file, 'r') as zObject:\n",
    "            zObject.extractall(path=self.dic.unzip_dir)\n",
    "        logger.info('Zip file extracted at {}'.format(self.dic.unzip_dir))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd432f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a41f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-16 15:57:33,438] @ [INFO] : common : Path to YAML config\\config.yaml loaded correctly\n",
      "[2023-08-16 15:57:33,440] @ [INFO] : common : Path to YAML params.yaml loaded correctly\n",
      "[2023-08-16 15:57:33,441] @ [INFO] : common : Directory artifacts created correctly\n",
      "[2023-08-16 15:57:33,443] @ [INFO] : common : Directory artifacts/data_ingestion created correctly\n",
      "[2023-08-16 15:57:33,444] @ [INFO] : <ipython-input-12-532bbc926797> : Data file artifacts/data_ingestion/summarizer-data.zip already exists, with File size: 7718.353515625 kB\n",
      "[2023-08-16 15:57:33,604] @ [INFO] : <ipython-input-12-532bbc926797> : Zip file extracted at artifacts/data_ingestion\n"
     ]
    }
   ],
   "source": [
    "# Update the pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    dic = config.getDataIngestionConfig()\n",
    "    di = DataIngestion(dic)\n",
    "\n",
    "    di.download_data()\n",
    "    di.extract_zip()\n",
    "except Exception as e:\n",
    "    raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044b734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39179754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe7022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "263d5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from ensure import ensure_annotations\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    data_dir: Path\n",
    "    status_dir: str\n",
    "    required_files: list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a079a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories, get_size\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ConfigurationManager:\n",
    "    config_file_path: Path = CONFIG_FILE_PATH\n",
    "    params_file_path: Path = PARAMS_FILE_PATH\n",
    "    \n",
    "    # Calling this to not override default init and post init is called as the last function of default init\n",
    "    def __post_init__(self):\n",
    "        self.config = read_yaml(self.config_file_path) # CONFIG_FILE_PATH taken from constants\n",
    "        self.params = read_yaml(self.params_file_path) # PARAMS_FILE_PATH taken from constants\n",
    "        \n",
    "        create_directories([self.config.artifacts_path, self.config.data_ingestion.root_dir])\n",
    "\n",
    "    def getDataValidationConfig(self) -> DataValidationConfig:\n",
    "        \n",
    "        data_validation_config = self.config.data_validation  \n",
    "        create_directories([data_validation_config.root_dir])\n",
    "        \n",
    "        dic = DataValidationConfig(root_dir = Path(data_validation_config.root_dir), \n",
    "                                   data_dir = Path(data_validation_config.data_dir),\n",
    "                                 status_dir = str(data_validation_config.status_dir), \n",
    "                                 required_files = data_validation_config.required_files)\n",
    "        \n",
    "        return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8587f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataValidation:\n",
    "    def __init__(self, dic : DataValidationConfig):\n",
    "        self.dic = dic\n",
    "    \n",
    "    def validate_data(self):\n",
    "        try:\n",
    "            # Validate files in folder, status is set to true only when all the required files are found\n",
    "            # Further validation should include column level, handling missing values, row level error fixing\n",
    "            list_of_files = os.listdir(os.path.join(self.dic.data_dir, 'samsum_dataset'))\n",
    "            data_validation_status = True\n",
    "\n",
    "            with open(self.dic.status_dir, 'w') as f:\n",
    "                f.write('Data validation status: {}'.format(data_validation_status))\n",
    "            \n",
    "#             print(self.dic.required_files)\n",
    "            for req_file in self.dic.required_files:\n",
    "                if(req_file not in list_of_files):\n",
    "                    print(req_file)\n",
    "                    data_validation_status = False\n",
    "                    with open(self.dic.status_dir, 'w') as f:\n",
    "                        f.write('Data validation status: {}'.format(data_validation_status))\n",
    "\n",
    "                logger.info('Data Validation status: {}'.format(data_validation_status))\n",
    "                return data_validation_status\n",
    "            \n",
    "            logger.info('Data Validation status: {}'.format(data_validation_status))\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            raise e\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c53395ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-16 22:09:39,308] @ [INFO] : common : Path to YAML config\\config.yaml loaded correctly\n",
      "[2023-08-16 22:09:39,312] @ [INFO] : common : Path to YAML params.yaml loaded correctly\n",
      "[2023-08-16 22:09:39,316] @ [INFO] : common : Directory artifacts created correctly\n",
      "[2023-08-16 22:09:39,316] @ [INFO] : common : Directory artifacts/data_ingestion created correctly\n",
      "[2023-08-16 22:09:39,321] @ [INFO] : common : Directory artifacts/data_validation created correctly\n",
      "[2023-08-16 22:09:39,324] @ [INFO] : <ipython-input-68-a47d829e1e0e> : Data Validation status: True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    dic = config.getDataValidationConfig()\n",
    "    di = DataValidation(dic)\n",
    "    di.validate_data()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d51382f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a3cae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_dir: Path\n",
    "    model_name: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3b8c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ConfigurationManager:\n",
    "    config_file_path: Path = CONFIG_FILE_PATH\n",
    "    params_file_path: Path = PARAMS_FILE_PATH\n",
    "    \n",
    "    # Calling this to not override default init and post init is called as the last function of default init\n",
    "    def __post_init__(self):\n",
    "        self.config = read_yaml(self.config_file_path) # CONFIG_FILE_PATH taken from constants\n",
    "        self.params = read_yaml(self.params_file_path) # PARAMS_FILE_PATH taken from constants\n",
    "        \n",
    "        create_directories([self.config.artifacts_path, self.config.data_ingestion.root_dir])\n",
    "        \n",
    "    def getDataTransformationConfig(self) -> DataTransformationConfig:\n",
    "\n",
    "        data_transformation_config = self.config.data_transformation\n",
    "\n",
    "        create_directories([data_transformation_config.root_dir])\n",
    "\n",
    "        dc = DataTransformationConfig(root_dir = Path(data_transformation_config.root_dir), \n",
    "                                   data_dir = Path(data_transformation_config.data_dir),\n",
    "                                 model_name = str(data_transformation_config.model_name))\n",
    "\n",
    "        return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d4c52c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\papu_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "import nltk\n",
    "import tqdm as tqdm\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, dc : DataTransformationConfig):\n",
    "        self.dc = dc\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.dc.model_name)\n",
    "    \n",
    "    \n",
    "    def tokenize_text_batches(self, batch_data):\n",
    "        input_encodings = self.tokenizer(text = batch_data['dialogue'], padding=True, truncation=True, max_length=1024)\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            target_encodings = self.tokenizer(text = batch_data['summary'], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        # Need to pass target encodings within the dict as labels key for the transformers model input\n",
    "        return {\n",
    "            'input_ids': input_encodings['input_ids'],\n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'labels': target_encodings['input_ids']\n",
    "        }\n",
    "\n",
    "\n",
    "    def transform_data(self):\n",
    "        try:\n",
    "            # Convert text to word encodings using autotokenizer of the pre-trained model\n",
    "            # dataset = Dataset.from_file(Path(self.dc.data_dir))\n",
    "            dataset = load_from_disk(Path(self.dc.data_dir))\n",
    "            data_enc = dataset.map(self.tokenize_text_batches, batched=True, batch_size = 500)\n",
    "            logger.info('Details about loaded data: {}'.format(data_enc))\n",
    "            return data_enc\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            raise e\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c2570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5bc8ed06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 02:53:21,992] @ [INFO] : common : Path to YAML config\\config.yaml loaded correctly\n",
      "[2023-08-17 02:53:21,994] @ [INFO] : common : Path to YAML params.yaml loaded correctly\n",
      "[2023-08-17 02:53:21,995] @ [INFO] : common : Directory artifacts created correctly\n",
      "[2023-08-17 02:53:21,997] @ [INFO] : common : Directory artifacts/data_ingestion created correctly\n",
      "[2023-08-17 02:53:21,998] @ [INFO] : common : Directory artifacts/data_transformation created correctly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 02:53:31,388] @ [INFO] : <ipython-input-119-d9942e7eb007> : Details about loaded data: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    dc = config.getDataTransformationConfig()\n",
    "    di = DataTransformation(dc)\n",
    "    di.transform_data()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ade42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2513d455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285de2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    model_name: str\n",
    "    output_dir: str\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    gradient_accumulation_steps: int\n",
    "    evaluation_strategy: str\n",
    "    predict_with_generate: bool\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5378d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories, get_size\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ConfigurationManager:\n",
    "    config_file_path: Path = CONFIG_FILE_PATH\n",
    "    params_file_path: Path = PARAMS_FILE_PATH\n",
    "    \n",
    "    # Calling this to not override default init and post init is called as the last function of default init\n",
    "    def __post_init__(self):\n",
    "        self.config = read_yaml(self.config_file_path) # CONFIG_FILE_PATH taken from constants\n",
    "        self.params = read_yaml(self.params_file_path) # PARAMS_FILE_PATH taken from constants\n",
    "        \n",
    "        create_directories([self.config.artifacts_path, self.config.data_ingestion.root_dir])\n",
    "\n",
    "    def getModelTrainerConfig(self) -> ModelTrainerConfig:\n",
    "        \n",
    "        # Take model name from config file\n",
    "        # Take other training arguments from params file\n",
    "        \n",
    "        model_trainer_params = self.params.TrainingArguments  \n",
    "        create_directories([Path(model_trainer_params.output_dir)])\n",
    "        \n",
    "        mtc = ModelTrainerConfig(model_name = self.config.model_name,\n",
    "                                  output_dir= Path(model_trainer_params.output_dir),\n",
    "                                  num_train_epochs= int(model_trainer_params.num_train_epochs),\n",
    "                                  warmup_steps = model_trainer_params.warmup_steps,\n",
    "                                  per_device_train_batch_size= model_trainer_params.per_device_train_batch_size,\n",
    "                                  per_device_eval_batch_size= model_trainer_params.per_device_eval_batch_size,\n",
    "                                  weight_decay= model_trainer_params.weight_decay,\n",
    "                                  logging_steps= model_trainer_params.logging_steps,\n",
    "                                  gradient_accumulation_steps= model_trainer_params.gradient_accumulation_steps,\n",
    "                                  evaluation_strategy = model_trainer_params.evaluation_strategy,\n",
    "                                  predict_with_generate= model_trainer_params.predict_with_generate)\n",
    "        \n",
    "        return mtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8f7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "563705d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\papu_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq            \n",
    "import evaluate\n",
    "import nltk\n",
    "import tqdm as tqdm\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, mtc : ModelTrainerConfig):\n",
    "        self.mtc = mtc\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.mtc.model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM(self.mtc.model_name)\n",
    "        self.metric = evaluate.load('rouge')\n",
    "    \n",
    "    \n",
    "    # Define metric\n",
    "    def compute_metrics(self, predictions):\n",
    "        # To decode the generated tokens to words\n",
    "        preds, labels = predictions\n",
    "        \n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces = True)\n",
    "        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces = True)\n",
    "        \n",
    "        # rougeLSum expects newline after each sentence\n",
    "        decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "#         prediction_summary = [self.tokenizer.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces = True) \n",
    "#                              for token in predictions]\n",
    "        \n",
    "        # Compute ROUGE score\n",
    "        return metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    \n",
    "    def train_model(self, data_transformed):\n",
    "        \n",
    "        training_args = TrainingArguments(output_dir = self.mtc.output_dir, \n",
    "                                          num_train_epochs = self.mtc.num_train_epochs,\n",
    "                                          warmup_steps = self.mtc.warmup_steps, \n",
    "                                          per_device_train_batch_size = self.mtc.per_device_train_batch_size,\n",
    "                                          per_device_eval_batch_size = self.mtc.per_device_eval_batch_size,\n",
    "                                          weight_decay = self.mtc.weight_decay,\n",
    "                                          logging_steps = self.mtc.logging_steps,\n",
    "                                          gradient_accumulation_steps= self.mtc.gradient_accumulation_steps,\n",
    "                                          evaluation_strategy = self.mtc.evaluation_strategy,\n",
    "                                          predict_with_generate= self.mtc.predict_with_generate)\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # Data collators are objects that will form a batch by using a list of dataset elements as input.\n",
    "        # These elements are of the same type as the elements of train_dataset or eval_dataset. \n",
    "        # To be able to build batches, data collators may apply some processing (like padding).\n",
    "        # Some of them (like DataCollatorForLanguageModeling) also apply some random data augmentation \n",
    "        # (like random masking) on the formed batch.\n",
    "        \n",
    "        seq2seq_dc = DataCollatorForSeq2Seq(tokenizer, model = model_mt)\n",
    "\n",
    "        trainer = Trainer(model = self.model.to(device), args = training_args, tokenizer = self.tokenizer,\n",
    "                          data_collator = seq2seq_dc, train_dataset = data_transformed['test'], \n",
    "                          eval_dataset = data_transformed['validation'], compute_metrics = self.compute_metrics)\n",
    "        \n",
    "#         trainer.train()\n",
    "\n",
    "        ## Save model after fine-tuning\n",
    "        self.model.save_pretrained(Path(self.mtc.output_dir))\n",
    "    \n",
    "        ## Save tokenizer after making it train on the fine tune dataset\n",
    "        self.tokenizer.save_pretrained(os.path.join(self.mtc.output_dir, 'tokenizer'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e5454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transformed to train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698eaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    dc = config.getModelTrainerConfig()\n",
    "    di = ModelTrainer(dc)\n",
    "    di.train_model(data_transformed)\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d35e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582379c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7337a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb3b6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    model_name: str\n",
    "    data_dir: Path\n",
    "    model_dir: Path\n",
    "    tokenizer_dir: Path\n",
    "    length_penalty: float\n",
    "    max_length: int\n",
    "    batch_size: int\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d36ce328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories, get_size\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ConfigurationManager:\n",
    "    config_file_path: Path = CONFIG_FILE_PATH\n",
    "    params_file_path: Path = PARAMS_FILE_PATH\n",
    "    \n",
    "    # Calling this to not override default init and post init is called as the last function of default init\n",
    "    def __post_init__(self):\n",
    "        self.config = read_yaml(self.config_file_path) # CONFIG_FILE_PATH taken from constants\n",
    "        self.params = read_yaml(self.params_file_path) # PARAMS_FILE_PATH taken from constants\n",
    "        \n",
    "        create_directories([self.config.artifacts_path, self.config.data_ingestion.root_dir])\n",
    "\n",
    "    def getModelEvaluationConfig(self) -> ModelEvaluationConfig:\n",
    "        \n",
    "        # Take model name from config file\n",
    "        # Take other training arguments from params file\n",
    "        \n",
    "        model_eval_config = self.config.model_evaluation  \n",
    "        \n",
    "        mec = ModelEvaluationConfig(\n",
    "                                model_name = model_eval_config.model_name,\n",
    "                                data_dir = Path(model_eval_config.data_dir),\n",
    "                                model_dir = Path(model_eval_config.model_dir),\n",
    "                                tokenizer_dir = Path(model_eval_config.tokenizer_dir),\n",
    "                                length_penalty = model_eval_config.length_penalty,\n",
    "                                max_length = model_eval_config.max_length,\n",
    "                                batch_size = model_eval_config.batch_size)\n",
    "        \n",
    "        return mec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b68416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a7a70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\papu_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq            \n",
    "import evaluate\n",
    "import nltk\n",
    "import tqdm as tqdm\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, mec : ModelEvaluationConfig):\n",
    "        self.mec = mec\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.mec.tokenizer_dir) # Load tokenizer from path not model name\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.mec.model_dir)  # Load model from path not model name\n",
    "        self.metric = evaluate.load('rouge')\n",
    "    \n",
    "    def eval_model(self):\n",
    "        dataset = load_from_disk(self.mec.data_dir)['test'][:10]\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        content_data = []\n",
    "        target_data = []\n",
    "#         print(dataset)\n",
    "        \n",
    "        for i in range(0, len(dataset['id']), self.mec.batch_size):\n",
    "            content_data.append(dataset['dialogue'][i: i+self.mec.batch_size])\n",
    "            target_data.append(dataset['summary'][i: i+self.mec.batch_size])\n",
    "\n",
    "        for content, target in tqdm.tqdm(zip(content_data, target_data), total = len(content_data)):\n",
    "            content_enc = self.tokenizer(text = content, padding=True, truncation=True, max_length=1024, return_tensors = 'pt')\n",
    "                \n",
    "            prediction_tokens = self.model.generate(input_ids = content_enc['input_ids'].to(device),\n",
    "                                           attention_mask = content_enc['attention_mask'].to(device),\n",
    "                                           length_penalty = self.mec.length_penalty, max_length = self.mec.max_length)\n",
    "            \n",
    "            \n",
    "            # To decode the generated tokens to words\n",
    "            prediction_summary = [self.tokenizer.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces = True) \n",
    "                                 for token in prediction_tokens]\n",
    "            \n",
    "         \n",
    "            print('origg preds: ', prediction_summary, '\\n')\n",
    "            print('orig labels: ',target, '\\n')\n",
    "            \n",
    "            # rougeLSum expects newline after each sentence\n",
    "            decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in prediction_summary]\n",
    "            decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in target]\n",
    "            \n",
    "            print('decoded preds: ', decoded_preds, '\\n')\n",
    "            print('decoded_labels: ',decoded_labels, '\\n')\n",
    "            \n",
    "            # Compute ROUGE score\n",
    "            self.metric.add_batch(predictions = decoded_preds, references= decoded_labels)\n",
    "            \n",
    "        result = self.metric.compute(use_stemmer=True)\n",
    "        print(result)\n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443b519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72c52c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-20 02:45:37,910] @ [INFO] : common : Path to YAML config\\config.yaml loaded correctly\n",
      "[2023-08-20 02:45:37,913] @ [INFO] : common : Path to YAML params.yaml loaded correctly\n",
      "[2023-08-20 02:45:37,914] @ [INFO] : common : Directory artifacts created correctly\n",
      "[2023-08-20 02:45:37,915] @ [INFO] : common : Directory artifacts/data_ingestion created correctly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                      | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origg preds:  [': Urgh.. Alright Hannah: Bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye', \": MACHINE! Rob: That's so gr8! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I know! Rob: I'll check. Rob: I'll watch them now! Rob: I'll watch them\", \"Lenny: Babe, can you help me with something? Bob: What's up? Bob: Send me photos Lenny: file_photo> Lenny: file_photo> Lenny: file_photo> Lenny: file_photo> Bob: I like the first ones best Lenny: But I already have purple trousers.\", \",,,,, what do you want for dinner tonight? Emma: gah, don't worry about it tonight Will: what do you mean? Emma: gah, don't worry about it tonight Will: what do you mean? Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry Will: Well what time will you be home? Emma: soon, hopefully Will: you sure? Emma: no no no it's alright. I\", \": ok for tea! Jane: I'm on my way to Morocco.. Ollie: ok for tea! Jane: I'm on my way..\", \",: I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina. Benjamin: Yeah, I guess so Daniel: I'm with Hilary atm and won't let go of her for the rest of the day. Benjamin: Yeah, I'm sooo tired after yesterday Hilary: Sounds good. We'll try to avoid talking about their subject of research.\", \":: file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> Max: I'll check them out. Thanks. Payton: Yes and no. Max: I like shopping?\", 'I hate my work. Tina: I hate my work. Tina: I hate my work. Tina: I hate my work. Tina: I hate my work. Tina: I hate my work.', \"Beatrice: I am in town, shopping. They have nice scarfs in the shop next to the church. Do you want one? Leo: No, thanks Beatrice: But you don't have a scarf. Leo: Because I don't need it. Leo: I don't care. You will get a scarf. Leo: How understanding of you! Beatrice: I've had enough. Leo: Eh.\", ': okay man, if you say so, Ivan: okay man, if you say so Ivan: yea just be there Eric: alright man, if you say so Ivan: yea just be there Eric: alright man, if you say so Ivan: yea just be there Eric: alright man, if you say so Ivan: alright man, if you say so Ivan: yea just be there Eric: alright man,'] \n",
      "\n",
      "orig labels:  [\"Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\", 'Eric and Rob are going to watch a stand-up on youtube.', \"Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\", 'Emma will be home soon and she will let Will know.', \"Jane is in Warsaw. Ollie and Jane has a party. Jane lost her calendar. They will get a lunch this week on Friday. Ollie accidentally called Jane and talked about whisky. Jane cancels lunch. They'll meet for a tea at 6 pm.\", \"Hilary has the keys to the apartment. Benjamin wants to get them and go take a nap. Hilary is having lunch with some French people at La Cantina. Hilary is meeting them at the entrance to the conference hall at 2 pm. Benjamin and Elliot might join them. They're meeting for the drinks in the evening.\", 'Payton provides Max with websites selling clothes. Payton likes browsing and trying on the clothes but not necessarily buying them. Payton usually buys clothes and books as he loves reading.', 'Rita and Tina are bored at work and have still 4 hours left.', \"Beatrice wants to buy Leo a scarf, but he doesn't like scarves. She cares about his health and will buy him a scarf no matter his opinion.\", \"Eric doesn't know if his parents let him go to Ivan's brother's wedding. Ivan will talk to them.\"] \n",
      "\n",
      "decoded preds:  [': Urgh.. Alright Hannah: Bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye', \": MACHINE!\\nRob: That's so gr8!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I know!\\nRob: I'll check.\\nRob: I'll watch them now!\\nRob: I'll watch them\", \"Lenny: Babe, can you help me with something?\\nBob: What's up?\\nBob: Send me photos Lenny: file_photo> Lenny: file_photo> Lenny: file_photo> Lenny: file_photo> Bob: I like the first ones best Lenny: But I already have purple trousers.\", \",,,,, what do you want for dinner tonight?\\nEmma: gah, don't worry about it tonight Will: what do you mean?\\nEmma: gah, don't worry about it tonight Will: what do you mean?\\nEmma: not really, but it's ok, don't worry about cooking though, I'm not hungry Will: Well what time will you be home?\\nEmma: soon, hopefully Will: you sure?\\nEmma: no no no it's alright.\\nI\", \": ok for tea!\\nJane: I'm on my way to Morocco.. Ollie: ok for tea!\\nJane: I'm on my way..\", \",: I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina.\\nBenjamin: Yeah, I guess so Daniel: I'm with Hilary atm and won't let go of her for the rest of the day.\\nBenjamin: Yeah, I'm sooo tired after yesterday Hilary: Sounds good.\\nWe'll try to avoid talking about their subject of research.\", \":: file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> file_other> Max: I'll check them out.\\nThanks.\\nPayton: Yes and no.\\nMax: I like shopping?\", 'I hate my work.\\nTina: I hate my work.\\nTina: I hate my work.\\nTina: I hate my work.\\nTina: I hate my work.\\nTina: I hate my work.', \"Beatrice: I am in town, shopping.\\nThey have nice scarfs in the shop next to the church.\\nDo you want one?\\nLeo: No, thanks Beatrice: But you don't have a scarf.\\nLeo: Because I don't need it.\\nLeo: I don't care.\\nYou will get a scarf.\\nLeo: How understanding of you!\\nBeatrice: I've had enough.\\nLeo: Eh.\", ': okay man, if you say so, Ivan: okay man, if you say so Ivan: yea just be there Eric: alright man, if you say so Ivan: yea just be there Eric: alright man, if you say so Ivan: yea just be there Eric: alright man, if you say so Ivan: alright man, if you say so Ivan: yea just be there Eric: alright man,'] \n",
      "\n",
      "decoded_labels:  [\"Hannah needs Betty's number but Amanda doesn't have it.\\nShe needs to contact Larry.\", 'Eric and Rob are going to watch a stand-up on youtube.', \"Lenny can't decide which trousers to buy.\\nBob advised Lenny on that topic.\\nLenny goes with Bob's advice to pick the trousers that are of best quality.\", 'Emma will be home soon and she will let Will know.', \"Jane is in Warsaw.\\nOllie and Jane has a party.\\nJane lost her calendar.\\nThey will get a lunch this week on Friday.\\nOllie accidentally called Jane and talked about whisky.\\nJane cancels lunch.\\nThey'll meet for a tea at 6 pm.\", \"Hilary has the keys to the apartment.\\nBenjamin wants to get them and go take a nap.\\nHilary is having lunch with some French people at La Cantina.\\nHilary is meeting them at the entrance to the conference hall at 2 pm.\\nBenjamin and Elliot might join them.\\nThey're meeting for the drinks in the evening.\", 'Payton provides Max with websites selling clothes.\\nPayton likes browsing and trying on the clothes but not necessarily buying them.\\nPayton usually buys clothes and books as he loves reading.', 'Rita and Tina are bored at work and have still 4 hours left.', \"Beatrice wants to buy Leo a scarf, but he doesn't like scarves.\\nShe cares about his health and will buy him a scarf no matter his opinion.\", \"Eric doesn't know if his parents let him go to Ivan's brother's wedding.\\nIvan will talk to them.\"] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-20 02:45:54,878] @ [INFO] : rouge_scorer : Using default tokenizer.\n",
      "{'rouge1': 0.17579455149472636, 'rouge2': 0.025573192239858912, 'rougeL': 0.13901103445030416, 'rougeLsum': 0.164741275343337}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    dc = config.getModelEvaluationConfig()\n",
    "    me = ModelEvaluation(dc)\n",
    "    me.eval_model()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4714a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3f6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5eb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0b9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a823f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72f27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e0581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a36e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dcb092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515ba77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0970484a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32580c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed289bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cdf840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c71d5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b441d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b662fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea8529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fcf4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a592f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4dea91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb817357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e5a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8247f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc798a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919fa8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa04dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8829e75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
