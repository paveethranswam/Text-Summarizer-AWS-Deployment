{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6baf8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import urllib\n",
    "from textSummarizer.logging import logger\n",
    "from zipfile import ZipFile\n",
    "from dataclasses import dataclass\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61b86df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\papu_\\\\OneDrive\\\\Desktop\\\\University Courses\\\\3rd Semester\\\\AWS ML Speciality\\\\Text-Summarizer-AWS-Deployment\\\\research'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1284b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07521ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\papu_\\\\OneDrive\\\\Desktop\\\\University Courses\\\\3rd Semester\\\\AWS ML Speciality\\\\Text-Summarizer-AWS-Deployment'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c176bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity.yaml\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_dir: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e86994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad = DataIngestionConfig('a', 'b', 'c', 'd')\n",
    "ad.unzip_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c2746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d80762f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories, get_size\n",
    "\n",
    "@dataclass\n",
    "class ConfigurationManager:\n",
    "    config_file_path: Path = CONFIG_FILE_PATH\n",
    "    params_file_path: Path = PARAMS_FILE_PATH\n",
    "    \n",
    "    # Calling this to not override default init and post init is called as the last function of default init\n",
    "    def __post_init__(self):\n",
    "        self.config = read_yaml(self.config_file_path) # CONFIG_FILE_PATH taken from constants\n",
    "        self.params = read_yaml(self.params_file_path) # PARAMS_FILE_PATH taken from constants\n",
    "        \n",
    "        create_directories([self.config.artifacts_path, self.config.data_ingestion.root_dir])\n",
    "        \n",
    "    def getDataIngestionConfig(self) -> DataIngestionConfig:\n",
    "        \n",
    "        data_ingestion_config = self.config.data_ingestion\n",
    "        \n",
    "        dic = DataIngestionConfig(root_dir = data_ingestion_config.root_dir, \n",
    "                                 source_dir = data_ingestion_config.source_dir, \n",
    "                                 local_data_file = data_ingestion_config.local_data_file, \n",
    "                                 unzip_dir = data_ingestion_config.unzip_dir)\n",
    "        \n",
    "        return dic\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c48ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc6c77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components creation\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, dic : DataIngestionConfig):\n",
    "        self.dic = dic\n",
    "        \n",
    "    # Download the data from url and save to the local file name and extract the zip contents, save them all in artifacts folder\n",
    "    def download_data(self):\n",
    "        if(not os.path.exists(self.dic.local_data_file)):\n",
    "            filename, header = urllib.request.urlretrieve(url = self.dic.source_dir, filename = self.dic.local_data_file)\n",
    "            logger.info('Data file {} downloaded, with return header {}'.format(filename, header))\n",
    "        else:\n",
    "            size = get_size(Path(self.dic.local_data_file))\n",
    "            logger.info('Data file {} already exists, with {}'.format(self.dic.local_data_file, size))\n",
    "    # Extract\n",
    "    def extract_zip(self):\n",
    "        with ZipFile(self.dic.local_data_file, 'r') as zObject:\n",
    "            zObject.extractall(path=self.dic.unzip_dir)\n",
    "        logger.info('Zip file extracted at {}'.format(self.dic.unzip_dir))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd432f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a41f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-16 15:57:33,438] @ [INFO] : common : Path to YAML config\\config.yaml loaded correctly\n",
      "[2023-08-16 15:57:33,440] @ [INFO] : common : Path to YAML params.yaml loaded correctly\n",
      "[2023-08-16 15:57:33,441] @ [INFO] : common : Directory artifacts created correctly\n",
      "[2023-08-16 15:57:33,443] @ [INFO] : common : Directory artifacts/data_ingestion created correctly\n",
      "[2023-08-16 15:57:33,444] @ [INFO] : <ipython-input-12-532bbc926797> : Data file artifacts/data_ingestion/summarizer-data.zip already exists, with File size: 7718.353515625 kB\n",
      "[2023-08-16 15:57:33,604] @ [INFO] : <ipython-input-12-532bbc926797> : Zip file extracted at artifacts/data_ingestion\n"
     ]
    }
   ],
   "source": [
    "# Update the pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    dic = config.getDataIngestionConfig()\n",
    "    di = DataIngestion(dic)\n",
    "\n",
    "    di.download_data()\n",
    "    di.extract_zip()\n",
    "except Exception as e:\n",
    "    raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044b734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39179754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe7022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "263d5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from ensure import ensure_annotations\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    data_dir: Path\n",
    "    status_dir: str\n",
    "    required_files: list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a079a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories, get_size\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ConfigurationManager:\n",
    "    config_file_path: Path = CONFIG_FILE_PATH\n",
    "    params_file_path: Path = PARAMS_FILE_PATH\n",
    "    \n",
    "    # Calling this to not override default init and post init is called as the last function of default init\n",
    "    def __post_init__(self):\n",
    "        self.config = read_yaml(self.config_file_path) # CONFIG_FILE_PATH taken from constants\n",
    "        self.params = read_yaml(self.params_file_path) # PARAMS_FILE_PATH taken from constants\n",
    "        \n",
    "        create_directories([self.config.artifacts_path, self.config.data_ingestion.root_dir])\n",
    "\n",
    "    def getDataValidationConfig(self) -> DataValidationConfig:\n",
    "        \n",
    "        data_validation_config = self.config.data_validation  \n",
    "        create_directories([data_validation_config.root_dir])\n",
    "        \n",
    "        dic = DataValidationConfig(root_dir = Path(data_validation_config.root_dir), \n",
    "                                   data_dir = Path(data_validation_config.data_dir),\n",
    "                                 status_dir = str(data_validation_config.status_dir), \n",
    "                                 required_files = data_validation_config.required_files)\n",
    "        \n",
    "        return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8587f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataValidation:\n",
    "    def __init__(self, dic : DataValidationConfig):\n",
    "        self.dic = dic\n",
    "    \n",
    "    def validate_data(self):\n",
    "        try:\n",
    "            # Validate files in folder, status is set to true only when all the required files are found\n",
    "            # Further validation should include column level, handling missing values, row level error fixing\n",
    "            list_of_files = os.listdir(os.path.join(self.dic.data_dir, 'samsum_dataset'))\n",
    "            data_validation_status = True\n",
    "\n",
    "            with open(self.dic.status_dir, 'w') as f:\n",
    "                f.write('Data validation status: {}'.format(data_validation_status))\n",
    "            \n",
    "#             print(self.dic.required_files)\n",
    "            for req_file in self.dic.required_files:\n",
    "                if(req_file not in list_of_files):\n",
    "                    print(req_file)\n",
    "                    data_validation_status = False\n",
    "                    with open(self.dic.status_dir, 'w') as f:\n",
    "                        f.write('Data validation status: {}'.format(data_validation_status))\n",
    "\n",
    "                logger.info('Data Validation status: {}'.format(data_validation_status))\n",
    "                return data_validation_status\n",
    "            \n",
    "            logger.info('Data Validation status: {}'.format(data_validation_status))\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            raise e\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c53395ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-16 22:09:39,308] @ [INFO] : common : Path to YAML config\\config.yaml loaded correctly\n",
      "[2023-08-16 22:09:39,312] @ [INFO] : common : Path to YAML params.yaml loaded correctly\n",
      "[2023-08-16 22:09:39,316] @ [INFO] : common : Directory artifacts created correctly\n",
      "[2023-08-16 22:09:39,316] @ [INFO] : common : Directory artifacts/data_ingestion created correctly\n",
      "[2023-08-16 22:09:39,321] @ [INFO] : common : Directory artifacts/data_validation created correctly\n",
      "[2023-08-16 22:09:39,324] @ [INFO] : <ipython-input-68-a47d829e1e0e> : Data Validation status: True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    dic = config.getDataValidationConfig()\n",
    "    di = DataValidation(dic)\n",
    "    di.validate_data()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d51382f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a3cae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_dir: Path\n",
    "    model_name: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3b8c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ConfigurationManager:\n",
    "    config_file_path: Path = CONFIG_FILE_PATH\n",
    "    params_file_path: Path = PARAMS_FILE_PATH\n",
    "    \n",
    "    # Calling this to not override default init and post init is called as the last function of default init\n",
    "    def __post_init__(self):\n",
    "        self.config = read_yaml(self.config_file_path) # CONFIG_FILE_PATH taken from constants\n",
    "        self.params = read_yaml(self.params_file_path) # PARAMS_FILE_PATH taken from constants\n",
    "        \n",
    "        create_directories([self.config.artifacts_path, self.config.data_ingestion.root_dir])\n",
    "        \n",
    "    def getDataTransformationConfig(self) -> DataTransformationConfig:\n",
    "\n",
    "        data_transformation_config = self.config.data_transformation\n",
    "\n",
    "        create_directories([data_transformation_config.root_dir])\n",
    "\n",
    "        dc = DataTransformationConfig(root_dir = Path(data_transformation_config.root_dir), \n",
    "                                   data_dir = Path(data_transformation_config.data_dir),\n",
    "                                 model_name = str(data_transformation_config.model_name))\n",
    "\n",
    "        return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d4c52c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\papu_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "import nltk\n",
    "import tqdm as tqdm\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, dc : DataTransformationConfig):\n",
    "        self.dc = dc\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.dc.model_name)\n",
    "    \n",
    "    \n",
    "    def tokenize_text_batches(self, batch_data):\n",
    "        input_encodings = self.tokenizer(text = batch_data['dialogue'], padding=True, truncation=True, max_length=1024)\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            target_encodings = self.tokenizer(text = batch_data['summary'], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        # Need to pass target encodings within the dict as labels key for the transformers model input\n",
    "        return {\n",
    "            'input_ids': input_encodings['input_ids'],\n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'labels': target_encodings['input_ids']\n",
    "        }\n",
    "\n",
    "\n",
    "    def transform_data(self):\n",
    "        try:\n",
    "            # Convert text to word encodings using autotokenizer of the pre-trained model\n",
    "            # dataset = Dataset.from_file(Path(self.dc.data_dir))\n",
    "            dataset = load_from_disk(Path(self.dc.data_dir))\n",
    "            data_enc = dataset.map(self.tokenize_text_batches, batched=True, batch_size = 500)\n",
    "            logger.info('Details about loaded data: {}'.format(data_enc))\n",
    "            return data_enc\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            raise e\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c2570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5bc8ed06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 02:53:21,992] @ [INFO] : common : Path to YAML config\\config.yaml loaded correctly\n",
      "[2023-08-17 02:53:21,994] @ [INFO] : common : Path to YAML params.yaml loaded correctly\n",
      "[2023-08-17 02:53:21,995] @ [INFO] : common : Directory artifacts created correctly\n",
      "[2023-08-17 02:53:21,997] @ [INFO] : common : Directory artifacts/data_ingestion created correctly\n",
      "[2023-08-17 02:53:21,998] @ [INFO] : common : Directory artifacts/data_transformation created correctly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 02:53:31,388] @ [INFO] : <ipython-input-119-d9942e7eb007> : Details about loaded data: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    dc = config.getDataTransformationConfig()\n",
    "    di = DataTransformation(dc)\n",
    "    di.transform_data()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ade42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2513d455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285de2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    model_name: str\n",
    "    output_dir: str\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    gradient_accumulation_steps: int\n",
    "    evaluation_strategy: str\n",
    "    predict_with_generate: bool\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5378d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories, get_size\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ConfigurationManager:\n",
    "    config_file_path: Path = CONFIG_FILE_PATH\n",
    "    params_file_path: Path = PARAMS_FILE_PATH\n",
    "    \n",
    "    # Calling this to not override default init and post init is called as the last function of default init\n",
    "    def __post_init__(self):\n",
    "        self.config = read_yaml(self.config_file_path) # CONFIG_FILE_PATH taken from constants\n",
    "        self.params = read_yaml(self.params_file_path) # PARAMS_FILE_PATH taken from constants\n",
    "        \n",
    "        create_directories([self.config.artifacts_path, self.config.data_ingestion.root_dir])\n",
    "\n",
    "    def getModelTrainerConfig(self) -> ModelTrainerConfig:\n",
    "        \n",
    "        # Take model name from config file\n",
    "        # Take other training arguments from params file\n",
    "        \n",
    "        model_trainer_params = self.params.TrainingArguments  \n",
    "        create_directories([Path(model_trainer_params.output_dir)])\n",
    "        \n",
    "        mtc = ModelTrainerConfig(model_name = self.config.model_name,\n",
    "                                  output_dir= Path(model_trainer_params.output_dir),\n",
    "                                  num_train_epochs= int(model_trainer_params.num_train_epochs),\n",
    "                                  warmup_steps = model_trainer_params.warmup_steps,\n",
    "                                  per_device_train_batch_size= model_trainer_params.per_device_train_batch_size,\n",
    "                                  per_device_eval_batch_size= model_trainer_params.per_device_eval_batch_size,\n",
    "                                  weight_decay= model_trainer_params.weight_decay,\n",
    "                                  logging_steps= model_trainer_params.logging_steps,\n",
    "                                  gradient_accumulation_steps= model_trainer_params.gradient_accumulation_steps,\n",
    "                                  evaluation_strategy = model_trainer_params.evaluation_strategy,\n",
    "                                  predict_with_generate= model_trainer_params.predict_with_generate)\n",
    "        \n",
    "        return mtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8f7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "563705d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\papu_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq            \n",
    "import evaluate\n",
    "import nltk\n",
    "import tqdm as tqdm\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, mtc : ModelTrainerConfig):\n",
    "        self.mtc = mtc\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.mtc.model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM(self.mtc.model_name)\n",
    "        self.metric = evaluate.load('rouge')\n",
    "    \n",
    "    \n",
    "    # Define metric\n",
    "    def compute_metrics(self, predictions):\n",
    "        # To decode the generated tokens to words\n",
    "        preds, labels = predictions\n",
    "        \n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces = True)\n",
    "        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces = True)\n",
    "        \n",
    "        # rougeLSum expects newline after each sentence\n",
    "        decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "#         prediction_summary = [self.tokenizer.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces = True) \n",
    "#                              for token in predictions]\n",
    "        \n",
    "        # Compute ROUGE score\n",
    "        return metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    \n",
    "    def train_model(self, data_transformed):\n",
    "        \n",
    "        training_args = TrainingArguments(output_dir = self.mtc.output_dir, \n",
    "                                          num_train_epochs = self.mtc.num_train_epochs,\n",
    "                                          warmup_steps = self.mtc.warmup_steps, \n",
    "                                          per_device_train_batch_size = self.mtc.per_device_train_batch_size,\n",
    "                                          per_device_eval_batch_size = self.mtc.per_device_eval_batch_size,\n",
    "                                          weight_decay = self.mtc.weight_decay,\n",
    "                                          logging_steps = self.mtc.logging_steps,\n",
    "                                          gradient_accumulation_steps= self.mtc.gradient_accumulation_steps,\n",
    "                                          evaluation_strategy = self.mtc.evaluation_strategy,\n",
    "                                          predict_with_generate= self.mtc.predict_with_generate)\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # Data collators are objects that will form a batch by using a list of dataset elements as input.\n",
    "        # These elements are of the same type as the elements of train_dataset or eval_dataset. \n",
    "        # To be able to build batches, data collators may apply some processing (like padding).\n",
    "        # Some of them (like DataCollatorForLanguageModeling) also apply some random data augmentation \n",
    "        # (like random masking) on the formed batch.\n",
    "        \n",
    "        seq2seq_dc = DataCollatorForSeq2Seq(tokenizer, model = model_mt)\n",
    "\n",
    "        trainer = Trainer(model = self.model.to(device), args = training_args, tokenizer = self.tokenizer,\n",
    "                          data_collator = seq2seq_dc, train_dataset = data_transformed['test'], \n",
    "                          eval_dataset = data_transformed['validation'], compute_metrics = self.compute_metrics)\n",
    "        \n",
    "#         trainer.train()\n",
    "\n",
    "        ## Save model after fine-tuning\n",
    "        self.model.save_pretrained(Path(self.mtc.output_dir))\n",
    "    \n",
    "        ## Save tokenizer after making it train on the fine tune dataset\n",
    "        self.tokenizer.save_pretrained(os.path.join(self.mtc.output_dir, 'tokenizer'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e5454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transformed to train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698eaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    dc = config.getModelTrainerConfig()\n",
    "    di = ModelTrainer(dc)\n",
    "    di.train_model(data_transformed)\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d35e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582379c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7337a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb3b6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    model_name: str\n",
    "    data_dir: Path\n",
    "    model_dir: Path\n",
    "    tokenizer_dir: Path\n",
    "    length_penalty: float\n",
    "    max_length: int\n",
    "    batch_size: int\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d36ce328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories, get_size\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ConfigurationManager:\n",
    "    config_file_path: Path = CONFIG_FILE_PATH\n",
    "    params_file_path: Path = PARAMS_FILE_PATH\n",
    "    \n",
    "    # Calling this to not override default init and post init is called as the last function of default init\n",
    "    def __post_init__(self):\n",
    "        self.config = read_yaml(self.config_file_path) # CONFIG_FILE_PATH taken from constants\n",
    "        self.params = read_yaml(self.params_file_path) # PARAMS_FILE_PATH taken from constants\n",
    "        \n",
    "        create_directories([self.config.artifacts_path, self.config.data_ingestion.root_dir])\n",
    "\n",
    "    def getModelEvaluationConfig(self) -> ModelEvaluationConfig:\n",
    "        \n",
    "        # Take model name from config file\n",
    "        # Take other training arguments from params file\n",
    "        \n",
    "        model_eval_config = self.config.model_evaluation  \n",
    "        \n",
    "        mec = ModelEvaluationConfig(\n",
    "                                model_name = model_eval_config.model_name,\n",
    "                                data_dir = Path(model_eval_config.data_dir),\n",
    "                                model_dir = Path(model_eval_config.model_dir),\n",
    "                                tokenizer_dir = Path(model_eval_config.tokenizer_dir),\n",
    "                                length_penalty = model_eval_config.length_penalty,\n",
    "                                max_length = model_eval_config.max_length,\n",
    "                                batch_size = model_eval_config.batch_size)\n",
    "        \n",
    "        return mec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b68416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5898578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\papu_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq            \n",
    "import evaluate\n",
    "import nltk\n",
    "import tqdm as tqdm\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, mec : ModelEvaluationConfig):\n",
    "        self.mec = mec\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.mec.tokenizer_dir) # Load tokenizer from path not model name\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.mec.model_dir)  # Load model from path not model name\n",
    "        self.metric = evaluate.load('rouge')\n",
    "    \n",
    "    def eval_model(self):\n",
    "        dataset = load_from_disk(self.mec.data_dir)['test'][:10]\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        content_data = []\n",
    "        target_data = []\n",
    "#         print(dataset)\n",
    "        \n",
    "        for i in range(0, len(dataset['id']), self.mec.batch_size):\n",
    "            content_data.append(dataset['dialogue'][i: i+self.mec.batch_size])\n",
    "            target_data.append(dataset['summary'][i: i+self.mec.batch_size])\n",
    "\n",
    "        for content, target in tqdm.tqdm(zip(content_data, target_data), total = len(content_data)):\n",
    "            content_enc = self.tokenizer(text = content, padding=True, truncation=True, max_length=1024, return_tensors = 'pt')\n",
    "                \n",
    "            prediction_tokens = self.model.generate(input_ids = content_enc['input_ids'].to(device),\n",
    "                                           attention_mask = content_enc['attention_mask'].to(device),\n",
    "                                           length_penalty = self.mec.length_penalty, max_length = self.mec.max_length)\n",
    "            \n",
    "            \n",
    "            # To decode the generated tokens to words\n",
    "            prediction_summary = [self.tokenizer.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces = True) \n",
    "                                 for token in prediction_tokens]\n",
    "            \n",
    "         \n",
    "            print('origg preds: ', prediction_summary, '\\n')\n",
    "            print('orig labels: ',target, '\\n')\n",
    "            \n",
    "            # rougeLSum expects newline after each sentence\n",
    "            decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in prediction_summary]\n",
    "            decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in target]\n",
    "            \n",
    "            print('decoded preds: ', decoded_preds, '\\n')\n",
    "            print('decoded_labels: ',decoded_labels, '\\n')\n",
    "            \n",
    "            # Compute ROUGE score\n",
    "            self.metric.add_batch(predictions = decoded_preds, references= decoded_labels)\n",
    "            \n",
    "        result = self.metric.compute(use_stemmer=True)\n",
    "        print(result)\n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f063b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94ce84fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-20 02:42:29,479] @ [INFO] : common : Path to YAML config\\config.yaml loaded correctly\n",
      "[2023-08-20 02:42:29,482] @ [INFO] : common : Path to YAML params.yaml loaded correctly\n",
      "[2023-08-20 02:42:29,483] @ [INFO] : common : Directory artifacts created correctly\n",
      "[2023-08-20 02:42:29,485] @ [INFO] : common : Directory artifacts/data_ingestion created correctly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                      | 0/1 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': Can't extract `str` to `Vec`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27084\\2088992334.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27084\\2088992334.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetModelEvaluationConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelEvaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mme\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27084\\974131676.py\u001b[0m in \u001b[0;36meval_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             target_summary = [self.tokenizer.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces = True) \n\u001b[1;32m---> 50\u001b[1;33m                                  for token in target_enc]\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'origg preds: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27084\\974131676.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             target_summary = [self.tokenizer.decode(token, skip_special_tokens=True, clean_up_tokenization_spaces = True) \n\u001b[1;32m---> 50\u001b[1;33m                                  for token in target_enc]\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'origg preds: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\papu_\\onedrive\\desktop\\university courses\\3rd semester\\aws ml speciality\\text-summarizer-aws-deployment\\textsum\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3511\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3512\u001b[0m             \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3513\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3514\u001b[0m         )\n\u001b[0;32m   3515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\papu_\\onedrive\\desktop\\university courses\\3rd semester\\aws ml speciality\\text-summarizer-aws-deployment\\textsum\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         clean_up_tokenization_spaces = (\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 'ids': Can't extract `str` to `Vec`"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    dc = config.getModelEvaluationConfig()\n",
    "    me = ModelEvaluation(dc)\n",
    "    me.eval_model()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0608c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baeddf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f3173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b7386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d165d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dde7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce79816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d88fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59315855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff9ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27022a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6cc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74154d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dddef77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaab1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c107ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb76232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88679959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027d473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9fca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e22fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504cb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58693a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81357dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a89d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f9690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6748fcc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621278c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
